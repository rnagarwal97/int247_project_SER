{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Speech Emotion Recognition</h1>\n",
    "<h3 align='center'>Machine Learning Project</h3>\n",
    "<h4 align='center'>by Aryan Agarwal (11707334)</h4>\n",
    "<br>\n",
    "<br>\n",
    "<p><b>Assignment Topic:</b> The project to recognize the emotion of the customer for telephony Customer care.</p>\n",
    "<br>\n",
    "<br>\n",
    "This project creates a <b>Multi Layer Perceptron Classifier model</b>. I have trained this model using Speech dataset <b>RAVDESS: Ryerson Audio-Visual Database of Emotional Speech and Song</b>. This dataset contains audio file containing speech of various actors which can be used to train a model to detect their emotion. The original dataset is about 24.5GB and can be downloaded from <a href=\"https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio\" target=\"_blank\">Kaggle</a>. For this project I have used a reduced dataset which contain only a part of original RAVDESS dataset. This reduced dataset can be downloaded from my <a href=\"https://drive.google.com/file/d/1v3XBuTWDNa4yCYbTcutro_CY7uQ2YJB6/view?usp=sharing\">Google Drive:</a>\n",
    "<br><br>\n",
    "https://drive.google.com/file/d/1v3XBuTWDNa4yCYbTcutro_CY7uQ2YJB6/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Library\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./img/librosa.png\" height=\"100\" width = \"200\"/>\n",
    "\n",
    "<b>LibROSA: </b>LibROSA is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in d:\\installed\\anaconda\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (0.43.1)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: joblib>=0.12 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (0.14.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (0.20.3)\n",
      "Requirement already satisfied: six>=1.3 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\installed\\anaconda\\lib\\site-packages (from librosa) (1.16.2)\n",
      "Requirement already satisfied: llvmlite>=0.28.0dev0 in d:\\installed\\anaconda\\lib\\site-packages (from numba>=0.43.0->librosa) (0.28.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\installed\\anaconda\\lib\\site-packages (from soundfile>=0.9.0->librosa) (1.12.2)\n",
      "Requirement already satisfied: pycparser in d:\\installed\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SoundFile:</b> SoundFile can read and write sound files. File reading/writing is supported through libsndfile, which is a free, cross-platform, open-source (LGPL) library for reading and writing many different sampled sound file formats that runs on many platforms including Windows, OS X, and Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in d:\\installed\\anaconda\\lib\\site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\installed\\anaconda\\lib\\site-packages (from soundfile) (1.12.2)\n",
      "Requirement already satisfied: pycparser in d:\\installed\\anaconda\\lib\\site-packages (from cffi>=1.0->soundfile) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/numpy.png\" height=\"100\" width = \"200\"/>\n",
    "\n",
    "<b>NumPy: </b>We will use the Python programming language for all assignments in this course. Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\installed\\anaconda\\lib\\site-packages (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/sklearn.png\" height=\"100\" width = \"200\"/>\n",
    "\n",
    "<b>Scikit-learn: </b>Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in d:\\installed\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\installed\\anaconda\\lib\\site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in d:\\installed\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in d:\\installed\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working\n",
    "\n",
    "### Dataset information\n",
    "The RAVDESS dataset contains audio clips (wav file) with name containing emotion in encoded form as \n",
    "\n",
    "    '01':'neutral',\n",
    "    '02':'calm',\n",
    "    '03':'happy',\n",
    "    '04':'sad',\n",
    "    '05':'angry',\n",
    "    '06':'fearful',\n",
    "    '07':'disgust',\n",
    "    '08':'surprise'\n",
    "    \n",
    "So if file name is 01-02-08.wav that mean this audio clip has neutral, calm and surprise emotion. This can act as target value.\n",
    "\n",
    "### Features Information\n",
    "We will extract MFCC, Chroma and Mel from the audio files. This will form features we will use.\n",
    "<br>\n",
    "<b>MFCC:</b>Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound. Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in automatic speech and speaker recognition. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope.\n",
    "<br>\n",
    "<b>Chroma: </b>Pertains to the 12 different pitch classes. Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.\n",
    "<br>\n",
    "<b>Mel: </b>Mel Spectrogram Frequency. This is spectogram to represent the sound in mathematical form so that it is easier to visualize.\n",
    "\n",
    "\n",
    "### Opening dataset and extracting Features\n",
    "Now we will open each audio file using <b>soundfile</b> library and extract features using <b>Librosa</b> library. We store these features into numpy array. We also store target value (which we get from filename) into another numpy array. We also split this dataset into train and test data.\n",
    "\n",
    "### Training Model and Predicting\n",
    "After that we create sklearn Multi Layer Perceptron Classifier, train it using training data and then try to predict the test data using test data. We also calculate accuracy using sklearn.metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from sound file like mfcc, chroma and mel\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    \n",
    "    # Open filename with soundfile\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        \n",
    "        # if features are present include it in result ndarray\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs= np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr = sample_rate).T, axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "            \n",
    "    # return result ndarray with all the features of that file        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion in the RAVDESS dataset\n",
    "\n",
    "# dictionary to decode encoded target into verbal emotions\n",
    "emotions = {\n",
    "    '01':'neutral',\n",
    "    '02':'calm',\n",
    "    '03':'happy',\n",
    "    '04':'sad',\n",
    "    '05':'angry',\n",
    "    '06':'fearful',\n",
    "    '07':'disgust',\n",
    "    '08':'surprised'\n",
    "}\n",
    "\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and extract the features for each sound file\n",
    "def load_data(test_size = 0.2):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    # selecting all filename using same pattern \n",
    "    for file in glob.glob(\".\\\\RAVDESS\\\\Actor_*\\\\*.wav\"):\n",
    "        \n",
    "        file_name = os.path.basename(file)\n",
    "        \n",
    "        # Splitting each emotion and decoding it in emotion using dictionary\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        \n",
    "        # calling extract feature function created above to get feature of file\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        \n",
    "        # appending features and target for all the file\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    \n",
    "    # returning train and test splitted dataset\n",
    "    return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "x_train, x_test, y_train, y_test = load_data(test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n"
     ]
    }
   ],
   "source": [
    "# Size of train and test data\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "# total features extracted\n",
    "print(\"Features extracted:\", x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(alpha = 0.01, batch_size = 256, epsilon = 1e-08, hidden_layer_sizes = (300,), learning_rate = 'adaptive', max_iter = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test set\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.85416666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "# Predicting emotion for one of audio file\n",
    "pred_file = r\".\\RAVDESS\\Actor_01\\03-01-08-02-02-02-01.wav\"\n",
    "file_feature = extract_feature(pred_file, mfcc=True, chroma=True, mel=True)\n",
    "file_feature = file_feature.reshape(1,180)\n",
    "print(model.predict(file_feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
